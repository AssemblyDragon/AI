{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83f5c207",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad02aae",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0f6ce1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # To Work with arrays\n",
    "import matplotlib.pyplot as plt  # To plot graphs\n",
    "import pandas as pd  # To make matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a7585e",
   "metadata": {},
   "source": [
    "## Importing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46569280",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('Data.csv')  # Creates a Data Frame containing the contents of our dataset\n",
    "X = dataset.iloc[:, :-1].values  # iloc => Locate indexes [rows, columns]. We need all features in this, i.e. all rows of first 3 columns\n",
    "y = dataset.iloc[:, -1].values  # values meaning we are taking all the variables. Dependent Variable here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58969833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Country   Age   Salary Purchased\n",
      "0   France  44.0  72000.0        No\n",
      "1    Spain  27.0  48000.0       Yes\n",
      "2  Germany  30.0  54000.0        No\n",
      "3    Spain  38.0  61000.0        No\n",
      "4  Germany  40.0      NaN       Yes\n",
      "5   France  35.0  58000.0       Yes\n",
      "6    Spain   NaN  52000.0        No\n",
      "7   France  48.0  79000.0       Yes\n",
      "8  Germany  50.0  83000.0        No\n",
      "9   France  37.0  67000.0       Yes\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "793202dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 nan]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' nan 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c34d358a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c850267",
   "metadata": {},
   "source": [
    "## Taking care of Missing value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ad1b69",
   "metadata": {},
   "source": [
    "Two ways:-\n",
    "1. Ignoring Missing Values\n",
    "2. Replacing with average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cc065e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer  # Sklearn has many scientific tools. In this case, we are using preprocessing tool to take care of our misisng values\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')  # Creating an instance/object of this class\n",
    "# First Argument - what is the missing values datatype \n",
    "# Second Argumnet - tells that we have to replace the missing values by mean values/avg values\n",
    "imputer.fit(X[:, 1:3])  # Fit method will calculate the average. It also only takes the numeric value\n",
    "X[:, 1:3] = imputer.transform(X[:, 1:3])  # Transform will apply the model to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54974a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 63777.77777777778]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' 38.77777777777778 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ba06eb",
   "metadata": {},
   "source": [
    "## Encoding Categorial Data\n",
    "\n",
    "This Dataset contains one column with categories => Countries.\n",
    "It would be difficult for the model to find correlations between this column and the dependent variable. So, we encode this category to numbers.\n",
    "\n",
    "One way is to encode France => 0, Spain => 1, Germany => 2.\n",
    "But since we have given numerical order between these 3 countries, our model will think that this order matters. Which is not the case, there is no relationship order between these three. \n",
    "\n",
    "Better Method is One Hot Encoder => In this, we create 3 columns (since there are 3 countries). Then we will assign vectors to each country. France will become 1 0 0, Spain => 0 1 0 and finally Germany => 0 0 1. So now, there is no numerical order between these countires (Since we now only have 0s and 1s).\n",
    "\n",
    "We will also have to replace the Purchased column by 0s and 1s. It's fine to do this for this column as it is a binary outcome (Outcomes that can take one of two values)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee335ec9",
   "metadata": {},
   "source": [
    "### 1. Encoding the Independent Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a65bcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ct = ColumnTransformer(transformers=[('encoding', OneHotEncoder(), [0])], remainder='passthrough')\n",
    "# First Argumnet => Transformers - to specify the kind of transformation and which indexes of the columns we want to transform\n",
    "# transformers = [(kind of transformation => encoding, what kind of encoding=> onehotencoder, indexes of the columns to encode)]\n",
    "# Second Argument => Remainder - the columns not to apply the transformation i.e. age and salary\n",
    "# passthrough => do nothing to them and keep them in the table\n",
    "# without it => it will only keep the 3 new created column and delete the others\n",
    "\n",
    "X = np.array(ct.fit_transform(X))\n",
    "# But the able does not return a numpy array which is neccassory for the learning model. \n",
    "# Fit and Train functions of the model requires a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ec4e239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0 0.0 0.0 44.0 72000.0]\n",
      " [0.0 0.0 1.0 27.0 48000.0]\n",
      " [0.0 1.0 0.0 30.0 54000.0]\n",
      " [0.0 0.0 1.0 38.0 61000.0]\n",
      " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
      " [1.0 0.0 0.0 35.0 58000.0]\n",
      " [0.0 0.0 1.0 38.77777777777778 52000.0]\n",
      " [1.0 0.0 0.0 48.0 79000.0]\n",
      " [0.0 1.0 0.0 50.0 83000.0]\n",
      " [1.0 0.0 0.0 37.0 67000.0]]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(X)\n",
    "print(type(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b2e0b8",
   "metadata": {},
   "source": [
    "### 2. Encoding the Dependent Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f69c6730",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48a10629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 1 1 0 1 0 1]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(y)\n",
    "print(type(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f56f66",
   "metadata": {},
   "source": [
    "#### Question\n",
    "Should Feature scaling be done before or after spliting the dataset?\n",
    "Answer => Feature Scaling must be applies AFTER splitting the dataset\n",
    "\n",
    "What:-\n",
    "1. Splitting => Split in 2 sets, one to train model on existing observations and one test set to evaluate performance based on new observations. These data is exactly like the data that we are gonna give in the future when we deploy the model\n",
    "\n",
    "2. Feature scaling => scale all variables so that they take values in the same scale. So as to prevent one feature being dominated by the other and resulting in it being neglected by the model.\n",
    "\n",
    "Why:-\n",
    "- Test set is supposed to be a brand new set just like actual futute observation that you will give to the model upon deployment.\n",
    "- This means, that the model is not supposed to have the test set.\n",
    "- Feature scaling requires one to have the mean/average of the features.\n",
    "- If we apply feature scaling before, we will get the mean/average of the test set.\n",
    "- This will result in the information leakage on the test data set, which you are not supposed to have."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e10b798",
   "metadata": {},
   "source": [
    "### 3. Splitting the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "996bb488",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)\n",
    "# Third arguemnt - Split Size - recommended 80%, 20%\n",
    "# Random state = 1 - for education purposes, so that we get the same split. It basically fixing the seed to get the same result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b80be95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 0.0 1.0 38.77777777777778 52000.0]\n",
      " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
      " [1.0 0.0 0.0 44.0 72000.0]\n",
      " [0.0 0.0 1.0 38.0 61000.0]\n",
      " [0.0 0.0 1.0 27.0 48000.0]\n",
      " [1.0 0.0 0.0 48.0 79000.0]\n",
      " [0.0 1.0 0.0 50.0 83000.0]\n",
      " [1.0 0.0 0.0 35.0 58000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ba3e082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 1.0 0.0 30.0 54000.0]\n",
      " [1.0 0.0 0.0 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a10942ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b987ca13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589fe986",
   "metadata": {},
   "source": [
    "### 4. Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed415c98",
   "metadata": {},
   "source": [
    "- We apply Feature Scaling to ensure that some features are not dominated by the model, which might result in only these features being considered for the prediction and the model ignoring the rest of the features.\n",
    "- Feature Scaling may not be needed to be applied in every model.\n",
    "- Like for example, in regression model, y = b0 + b1.x1 + b2.x2 + ...., so if any feature (x) has a great value, its respective coefficient might become very low so as to componsate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16abc0c8",
   "metadata": {},
   "source": [
    "Two feature scaling methods:-\n",
    "\n",
    "1. Standardisation - Works all the time\n",
    "\n",
    "    - x_stand =  ( x - mean(x) ) / standard deviation x\n",
    "\n",
    "    - Value -3 to +3\n",
    "    \n",
    "\n",
    "2. Normalization - Recommeneded when we have a normal distribution in most of the features\n",
    "\n",
    "    - x_norm = ( x - min(x) ) / ( max(x) - min(x) )\n",
    "\n",
    "    - Value 0 to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbdd817",
   "metadata": {},
   "source": [
    "Now next question: Do we need to apply feature scaling to dummy variables (variables obtained after applying one hot endcoder)\n",
    "Answer : No\n",
    "\n",
    "1. Feature Scaling makes the values in the same range. Standardisation will transform values to be between -3 and +3. Since dummpy values are already between this range (they take values 0 and 1), we don't need to apply feature scaling to those columns.\n",
    "\n",
    "\n",
    "2. Not only that, we will also lose the interpretation. Since, right now, by looking we can tell which country it is. After feature scaling, we wont be able to easily tell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a3ca505",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train[:, 3:] = sc.fit_transform(X_train[:, 3:])\n",
    "X_test[:, 3:] = sc.transform(X_test[:, 3:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52affb3c",
   "metadata": {},
   "source": [
    "Fit => calculates mean and standard deviation\n",
    "\n",
    "Transform => Actually transforms the features\n",
    "\n",
    "- For training data, we need to calculate the mean and SD and apply it.\n",
    "- For testing data, we need to transform the values to be in the same range (i.e. -3 to +3), but we cannot take calculate their mean and SD, since they are some future data we dont know about. Morever, we need to apply the same range, so that the model does not gets confused. The model is trained on the some scaler. So for accurate result, we need to give it the same scaler, to predict accurate output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2906e790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 0.0 1.0 -0.19159184384578545 -1.0781259408412425]\n",
      " [0.0 1.0 0.0 -0.014117293757057777 -0.07013167641635372]\n",
      " [1.0 0.0 0.0 0.566708506533324 0.633562432710455]\n",
      " [0.0 0.0 1.0 -0.30453019390224867 -0.30786617274297867]\n",
      " [0.0 0.0 1.0 -1.9018011447007988 -1.420463615551582]\n",
      " [1.0 0.0 0.0 1.1475343068237058 1.232653363453549]\n",
      " [0.0 1.0 0.0 1.4379472069688968 1.5749910381638885]\n",
      " [1.0 0.0 0.0 -0.7401495441200351 -0.5646194287757332]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b81395a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 1.0 0.0 -1.4661817944830124 -0.9069571034860727]\n",
      " [1.0 0.0 0.0 -0.44973664397484414 0.2056403393225306]]\n"
     ]
    }
   ],
   "source": [
    "print(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
